{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8755a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from class_token import Morph, Token # important: The loader needs the Token and Morph classes defined in the same scope, or pickle can't reconstitute them.\n",
    "\n",
    "def load_sentences(pkl_file=\"oga_sentences.pkl\"):\n",
    "    \"\"\"\n",
    "    Load the list of sentences (each a list of Token objects)\n",
    "    from a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pkl_file : str\n",
    "        Path to the .pkl file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of sentences, where each sentence is a list of Token objects\n",
    "    \"\"\"\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        sentences = pickle.load(f)\n",
    "    return sentences\n",
    "\n",
    "sentences = load_sentences(\"oga_sentences_batch_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544fcd72",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrc_macronizer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclass_macronizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Macronizer\n\u001b[32m      3\u001b[39m macronizer = Macronizer(lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m output = \u001b[43mmacronizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33moga_batch_1_macronized.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      7\u001b[39m     f.write(output.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/grc-macronizer-oga/grc_macronizer/class_macronizer.py:158\u001b[39m, in \u001b[36mMacronizer.macronize\u001b[39m\u001b[34m(self, text, genre)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmacronize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, genre=\u001b[33m'\u001b[39m\u001b[33mprose\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Macronization is a modular and recursive process comprised of the following 13 steps, \u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    with the high-trust db modules first, then the algorithmic modules, the recursive ones and finally the hypotactic db module:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m \u001b[33;03m    My design goal is that it should be easy for the \"power user\" to change the order of the other modules, and to graft in new ones.\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     text_object = \u001b[43mText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowercase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlowercase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     token_lemma_pos_morph = text_object.token_lemma_pos_morph \u001b[38;5;66;03m# format: [[orth, token.lemma_, token.pos_, token.morph], ...]\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# lists to keep track of the modules' efficacy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/grc-macronizer-oga/grc_macronizer/class_text.py:172\u001b[39m, in \u001b[36mText.__init__\u001b[39m\u001b[34m(self, sentences, genre, debug, lowercase)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    168\u001b[39m     orth \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m diagnostic_word_list\n\u001b[32m    169\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m orth \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mἂν\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mἄν\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    170\u001b[39m ):\n\u001b[32m    171\u001b[39m     fail_counter += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[43mlogging\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWord \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43morth\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m not in diagnostic word list. Skipping.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Skip words without dichrona\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/logging/__init__.py:2225\u001b[39m, in \u001b[36mdebug\u001b[39m\u001b[34m(msg, *args, **kwargs)\u001b[39m\n\u001b[32m   2222\u001b[39m         basicConfig()\n\u001b[32m   2223\u001b[39m     root.info(msg, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdebug\u001b[39m(msg, *args, **kwargs):\n\u001b[32m   2226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2227\u001b[39m \u001b[33;03m    Log a message with severity 'DEBUG' on the root logger. If the logger has\u001b[39;00m\n\u001b[32m   2228\u001b[39m \u001b[33;03m    no handlers, call basicConfig() to add a console handler with a pre-defined\u001b[39;00m\n\u001b[32m   2229\u001b[39m \u001b[33;03m    format.\u001b[39;00m\n\u001b[32m   2230\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(root.handlers) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from grc_macronizer.class_macronizer import Macronizer\n",
    "\n",
    "macronizer = Macronizer(lowercase=True)\n",
    "output = macronizer.macronize(sentences)\n",
    "\n",
    "with open(\"oga_batch_1_macronized.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(output.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
